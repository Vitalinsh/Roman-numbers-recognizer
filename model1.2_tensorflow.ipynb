{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_data_to_mem, augmentation, show_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\".\", \"data\", \"train\")\n",
    "valid_path = os.path.join(\".\", \"data\", \"val\")\n",
    "test_path = os.path.join(\".\", \"data\", \"test\")\n",
    "\n",
    "classes = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\", \"VIII\"]\n",
    "class_mapping = {\"I\": 0, \"II\": 1, \"III\": 2, \"IV\": 3,\n",
    "                 \"V\": 4, \"VI\": 5, \"VII\": 6, \"VIII\":7}\n",
    "n_classes = len(classes)\n",
    "\n",
    "def class_to_number(n):\n",
    "    return class_mapping[n]\n",
    "\n",
    "img_rows, img_cols, n_channels = 64, 64, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data_to_mem(train_path, classes)\n",
    "X_valid, y_valid = load_data_to_mem(valid_path, classes)\n",
    "X_test, y_test = load_data_to_mem(test_path, classes)\n",
    "\n",
    "X_train, y_train = augmentation(X_train, y_train, n_transform=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_valid = np.array(X_valid, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# expand dimensions for CNN\n",
    "X_train = np.expand_dims(X_train, axis=3) \n",
    "X_valid = np.expand_dims(X_valid, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "y_train = np.array(list(map(class_to_number, y_train)))\n",
    "y_valid = np.array(list(map(class_to_number, y_valid)))\n",
    "y_test = np.array(list(map(class_to_number, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check shapes: \n",
      "X_train = (47940, 64, 64, 1)  Y_train = 47940\n",
      "X_valid = (311, 64, 64, 1)  Y_valid = 311\n",
      "X_test = (321, 64, 64, 1)  Y_test = 321\n"
     ]
    }
   ],
   "source": [
    "print(\"Check shapes: \")\n",
    "print(\"X_train =\", X_train.shape, \" Y_train =\", len(y_train))\n",
    "print(\"X_valid =\", X_valid.shape, \" Y_valid =\", len(y_valid))\n",
    "print(\"X_test =\", X_test.shape, \" Y_test =\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "#parameters of convolutional layer\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 5\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 5\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "#parameters of pooling layer\n",
    "pool2_fmaps = conv2_fmaps\n",
    "#parameters of fully connected network and outputs\n",
    "n_dense1 = 1024\n",
    "n_outputs = 8\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, img_rows, img_cols, n_channels], name = \"X\")\n",
    "y = tf.placeholder(tf.int32, shape = [None], name = \"y\")\n",
    "    \n",
    "conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size = conv1_ksize,\n",
    "                         strides = conv1_stride, padding=conv1_pad,\n",
    "                         activation = tf.nn.relu, name=\"conv_1\")\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\", name=\"pool_1\")\n",
    "# pool1 = tf.nn.dropout(pool1, keep_prob=0.8, name=\"dropout_1\")\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv_2\")\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\", name=\"pool_2\")\n",
    "# pool2 = tf.nn.dropout(pool2, keep_prob=0.8, name=\"dropout_2\")\n",
    "    \n",
    "conv3 = tf.layers.conv2d(pool2, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv_3\")\n",
    "pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1],  strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\", name=\"pool_3\")\n",
    "# pool3 = tf.nn.dropout(pool3, keep_prob=0.8, name=\"dropout_3\")\n",
    "\n",
    "flat1 = tf.layers.flatten(pool3, name=\"flatten_1\")\n",
    "dense1 = tf.layers.dense(flat1, n_dense1, activation=tf.nn.relu,\n",
    "                          name = \"dense_1\")\n",
    "# dense1 = tf.nn.dropout(dense1, keep_prob=0.5, name=\"dropout_4\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(dense1, n_outputs, name = \"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train accuracy: 0.375 Val accuracy: 0.21221864\n",
      "Epoch: 2 Train accuracy: 0.453125 Val accuracy: 0.2636656\n",
      "Epoch: 3 Train accuracy: 0.390625 Val accuracy: 0.27974278\n",
      "Epoch: 4 Train accuracy: 0.78125 Val accuracy: 0.6463022\n",
      "Epoch: 5 Train accuracy: 0.71875 Val accuracy: 0.70418006\n",
      "Epoch: 6 Train accuracy: 0.859375 Val accuracy: 0.7749196\n",
      "Epoch: 7 Train accuracy: 0.828125 Val accuracy: 0.78778136\n",
      "Epoch: 8 Train accuracy: 0.859375 Val accuracy: 0.77813506\n",
      "Epoch: 9 Train accuracy: 0.90625 Val accuracy: 0.77813506\n",
      "Epoch: 10 Train accuracy: 0.90625 Val accuracy: 0.7717042\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            X_batch = X_train[iteration * batch_size: iteration * batch_size + batch_size]\n",
    "            y_batch = y_train[iteration * batch_size: iteration * batch_size + batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch:\", epoch+1, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_valid)\n",
    "        save_path = saver.save(sess, \"./saved_models/model1.2_tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test):\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init) \n",
    "        saver = tf.train.import_meta_graph('saved_models/model1.2_tf.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./saved_models/'))\n",
    "        pred = sess.run([accuracy, loss], feed_dict={X: X_test, y: y_test})\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model1.2_tf\n",
      "[0.78193146, 1.0301605]\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
